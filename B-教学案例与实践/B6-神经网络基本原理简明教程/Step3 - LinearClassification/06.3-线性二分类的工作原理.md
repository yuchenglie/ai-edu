Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 6.3 线性二分类原理

### 6.3.1 线性分类和线性回归的异同

此原理对线性和非线性二分类都适用。

回忆一下前面学习过的线性回归，通过用均方差函数的误差反向传播的方法，不断矫正拟合直线的角度（Weights）和偏移（Bias），因为均方差函数能够准确地反映出当前的拟合程度。那么在线性分类中，我们能不能采取类似的方法呢？

线性分类，试图在含有两种样本的空间中划出一条分界线，让双方截然分开，就好像是中国象棋的棋盘中的楚河汉界一样。与线性回归相似的地方是，两者都需要划出那条“直线”来，但是不同的地方也很明显：

||线性回归|线性分类|
|---|---|---|
|相同点|需要在样本群中找到一条直线|需要在样本群中找到一条直线|
|不同点|用直线来拟合所有样本，使得各个样本到这条直线的距离尽可能最短|用直线来分割所有样本，使得正例样本和负例样本尽可能分布在直线两侧|

可以看到线性回归中的目标--“距离最短”，还是很容易理解的，但是线性分类的目标--“分布在两侧”，用数学方式如何描述呢？我们可以有代数和几何两种方式来描述：

- 代数方式：通过一个分类函数计算所有样本点在经过线性变换后的概率值，使得正例样本的概率大于0.5，而负例样本的概率小于0.5
- 几何方式：下图中，让所有正例样本处于直线的上方，所有负例样本处于直线的下方

<img src="..\Images\6\linear_binary_analysis.png">

### 6.3.2 二分类过程

下面我们以单样本双特征值为例来说明神经网络的二分类过程，这是用代数方式来解释其工作原理。

1. 正向计算

$$
z = x_1 w_1+ x_2 w_2 + b  \tag{1}
$$

2. 分类计算

$$
a={1 \over 1 + e^{-z}} \tag{2}
$$

3. 损失函数计算

$$
loss = -[y \ln (a)+(1-y) \ln (1-a)] \tag{3}
$$

### 6.3.3 数值计算举例

用下图举例来说明计算过程：

<img src="../Images/6/sample.png">

假设：

1. 绿色点为正例，标签值y=1，坐标值为(1, 3)
2. 红色点为负例，标签值y=0，坐标值分别为(1.5, 3)和(3, 1.5)
3. 假设公式1的初始值$w_1=-1，w_2=1，b=0$，所以有$z=-1\cdot x_1+1 \cdot x_2 + 0$，即$z=-x_1+x_2$。如果令z=0，即Logistic(z)=0.5，是两个区域的分割线（面），则有平面上的直线$x_2=x_1$

在上面三个点中，绿色点和右下角的红色点，是分类正确的点，而中间的红色点是分类错误的点。下面我们把公式1，2，3代入具体的值，来看一下神经网络如何做分类的。

1. 把绿色点代入公式，此时y=1：

$$z=-1+3=2 \tag{见公式1}$$
$$a=\frac{1}{1+e^{-2}}=0.88 \tag{见公式2}$$
$$loss=-[1 \cdot \ln 0.88 + 0]=0.127 \tag{见公式3}$$

2. 把左上方的红色点代入公式，此时y=0：

$$z=-1.5+3=1.5 \tag{见公式1}$$
$$a=\frac{1}{1+e^{-1.5}}=0.817 \tag{见公式2}$$
$$loss=-[0 + (1-0) \ln (1-0.817)]=1.7 \tag{见公式3}$$

3. 把右下方的红色点代入公式，此时y=0：

$$z=-3+1.5=-1.5 \tag{见公式1}$$
$$a=\frac{1}{1+e^{1.5}}=0.182 \tag{见公式2}$$
$$loss=-[0 + (1-0) \ln (1-0.182)]=0.2 \tag{见公式3}$$

对比总结如下表：

|点ID|颜色|坐标值|z值|a值|loss值|y值|分类情况|
|---|---|---|---|---|---|---|---|
|1|红色|(1,3)|2|0.88|0.127|1|正确|
|2|绿色|(1.5,3)|1.5|0.817|1.7|0|错误|
|3|绿色|(3,1.5)|-1.5|0.182|0.2|0|正确|

- 在正例情况y=1时，a如果越靠近1，表明分类越正确，此时损失值会越小。点1就是这种情况：a=0.88，距离1不远；loss值0.127，不算很大
- 在负例情况y=0时，a如果越靠近0，表明分类越正确，此时损失值会越小。点3就是这种情况：a=0.182，距离0不远；loos值0.2，不算很大
- 点2是分类错误的情况，a=0.817，应该是小于0.5的，却距离0远，距离1反而近，所以它的loss=1.7，从与其它两个点比较的相对值来看，是非常大的，这样误差就大，反向传播的力度也大

### 6.3.4 二分类的几何原理

我们再观察一下下面这张分类正确的图：

<img src="..\Images\6\linear_binary_analysis.png">

假设绿色方块为正类：标签值$y=1$，红色三角形为负类：标签值$y=0$。

从几何关系上理解，如果我们有一条直线，其公式为：$z = w \cdot x_1+b$，如图中的虚线所示，则所有正类的样本的x2都大于z，而所有的负类样本的x2都小于z，那么这条直线就是我们需要的分割线。用正例的样本来表示：

$$
x_2 > z，即正例满足条件：x_2 > w \cdot x_1 + b \tag{4}
$$

那么神经网络用矩阵运算+分类函数+损失函数这么复杂的流程，其工作原理是什么呢？

经典机器学习中的SVM确实就是用这种思路来解决这个问题的，即一个类别的所有样本在分割线的一侧，而负类样本都在线的另一侧。神经网络的正向公式如公式1，2所示，当a>0.5时，判为正类。当a<0.5时，判为负类。z=0即a=0.5时为分割线：

<img src="..\Images\6\sigmoid_binary.png">

我们用正例来举例：

$$a = Logistic(z) = {1 \over 1 + e^{-z}} > 0.5$$

做公式变形，再两边取自然对数，可以得到：

$$z > 0$$

即：
$$
z = x_1 \cdot w_1 + x_2 \cdot w_2 + b > 0
$$
对上式做一下变形，把x2放在左侧，其他项放在右侧：
$$
x_2 > - {w_1 \over w_2}x_1 - {b \over w_2} \tag{5}
$$
简化一下两个系数，令w'=-w1/w2，b'=-b/w2：
$$
x_2 > w' \cdot x_1 + b' \tag{6}
$$
比较一下公式4和6，
$$
x_2 > w \cdot x_1 + b \tag{4}
$$

一模一样！这就说明神经网络的工作原理和我们在二维平面上的直观感觉是相同的，即当x2大于一条直线时，会被判为正例。如果用负例举例，其结果将会是：$a<0.5，z<0，x_2 \lt w \cdot x_1 + b$，即点处于直线下方。

由此，我们还得到了一个额外的收获，即：

$$w' = - w1 / w2 \tag{7}$$

$$b' = -b/w2 \tag{8}$$

我们可以使用神经网络计算出$w1，w2，b$三个值以后，换算成$w'，b'$，以便在二维平面上画出分割线，来直观地判断神经网络训练结果的正确性。
