Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 10.2 非线性二分类实现

### 10.2.1 定义神经网络结构

<img src='../Images/10/xor_nn.png'/>

- 输入层两个特征值x1, x2
  $$
  x=\begin{pmatrix}
    x_1 & x_2
  \end{pmatrix}
  $$
- 隐层2x2的权重矩阵W1
$$
  W1=\begin{pmatrix}
    w_{11} & w_{12} \\
    w_{21} & w_{22} 
  \end{pmatrix}
$$
- 隐层1x2的偏移矩阵B1

$$
  B1=\begin{pmatrix}
    b_{11} & b_{12}
  \end{pmatrix}
$$

- 隐层由两个神经元构成
- 输出层2x1的权重矩阵W2
$$
  W2=\begin{pmatrix}
    w_{11} \\
    w_{21}  
  \end{pmatrix}
$$

- 输出层1x1的偏移矩阵B2

$$
  B2=\begin{pmatrix}
    b_{11}
  \end{pmatrix}
$$

- 输出层有一个神经元使用Logisitc函数进行分类

对于一般的用于二分类的双层神经网络可以是这样子的：

<img src='../Images/10/binary_classifier.png' width="600"/>

输入特征值可以有很多，隐层单元也可以有很多，输出单元只有一个，且后面要接Logistic分类函数和二分类交叉熵损失函数。

### 10.2.2 前向计算

根据网络结构，我们有了前向计算图：

<img src='../Images/10/binary_forward.png'/>

第一层：

$$
z_{11} = x_{11} w_{11} + x_{12} w_{21} + b_{11} \\
z_{12} = x_{11} w_{12} + x_{12} w_{22} + b_{12} \\
z1=\begin{pmatrix}
  z_{11} & z_{12}
\end{pmatrix}
\tag{8}
$$
$$
a_{11} = Sigmoid(z_{11}) \\
a_{12} = Sigmoid(z_{12}) \\
a1=\begin{pmatrix}
  a_{11} & a_{12}
\end{pmatrix} \tag{9}
$$

第二层：

$$
z2 = a_{11} w_{11} + a_{12} w_{21} + b_{11} \tag{10}
$$
$$a2 = Logistic(z2) \tag{11}$$

#### 损失函数

我们把异或问题归类成二分类问题，所以使用二分类交叉熵损失函数：

$$
loss = -y \ln a2 + (1-y) \ln (1-a2) \tag{12}
$$

### 10.2.3 反向传播

<img src='../Images/10/binary_backward.png'/>

#### 求损失函数对输出层的反向误差

对损失函数求导，可以得到损失函数对输出层的梯度值，即上图中的Z2部分。

根据公式15，求A2和Z2的导数：

$$

{\partial loss \over \partial z2}={\partial loss \over \partial a2}{\partial a2 \over \partial z2}
$$
$$
={a2-y \over a2(1-a2)} \cdot a2(1-a2)
$$
$$
=a2-y => dZ2 \tag{13}
$$

#### 求W2和B2的梯度

$$
{\partial loss \over \partial W2}=\begin{pmatrix}
  {\partial loss \over \partial w_{11}} \\
  \\
  {\partial loss \over \partial w_{21}}
\end{pmatrix}
=\begin{pmatrix}
  {\partial loss \over \partial z2}{\partial z2 \over \partial w_{11}} \\
  \\
  {\partial loss \over \partial z2}{\partial z2 \over \partial w_{21}}
\end{pmatrix}
$$
$$
=\begin{pmatrix}
  (a2-y)a_{11} \\
  (a2-y)a_{12} 
\end{pmatrix}
=\begin{pmatrix}
  a_{11} \\ a_{12}
\end{pmatrix}(a2-y)
$$
$$
=a1^T \cdot dZ2 => dW2  \tag{14}
$$
$${\partial{loss} \over \partial{B2}}=dZ2 => dB2 \tag{15}$$

#### 求损失函数对隐层的反向误差

$$
\frac{\partial{loss}}{\partial{a1}} = \begin{pmatrix}
  {\partial loss \over \partial a_{11}} & {\partial loss \over \partial a_{12}} 
\end{pmatrix}
$$
$$
=\begin{pmatrix}
\frac{\partial{loss}}{\partial{z2}}  \frac{\partial{z2}}{\partial{a_{11}}} & \frac{\partial{loss}}{\partial{z2}}  \frac{\partial{z2}}{\partial{a_{12}}}  
\end{pmatrix}
$$
$$
=\begin{pmatrix}
dZ2 \cdot w_{11} & dZ2 \cdot w_{21}
\end{pmatrix}
$$
$$
=dZ2 \cdot \begin{pmatrix}
  w_{11} & w_{21}
\end{pmatrix}
$$
$$
=dZ2 \cdot W2^T \tag{16}
$$

$$
{\partial a1 \over \partial z1}=a1 \odot (1-a1) => dA1\tag{17}
$$

所以最后到达z1的误差矩阵是：

$$
{\partial loss \over \partial z1}={\partial loss \over \partial a1}{\partial a1 \over \partial z1}
$$
$$
=dZ2 \cdot W2^T \odot dA1 => dZ1 \tag{18}
$$

有了dZ1后，再向前求W1和B1的误差，就和第5章中一样了，我们直接列在下面：

$$
dW1=X^T \cdot dZ1 \tag{19}
$$
$$
dB1=dZ1 \tag{20}
$$
