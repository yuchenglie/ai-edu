Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 丢弃法 Dropout

2012年，Alex、Hinton在其论文《ImageNet Classification with Deep Convolutional Neural Networks》中用到了Dropout算法，用于防止过拟合。

我们假设原来的神经网络是这个结构，最后输出三分类结果：

<img src="..\Images\16\dropout_before.png">

Dropout可以作为训练深度神经网络的一种正则方法供选择。在每个训练批次中，通过忽略一部分的神经元（让其隐层节点值为0），可以明显地减少过拟合现象。这种方式可以减少隐层节点间的相互作用，高层的神经元需要低层的神经元的输出才能发挥作用，如果高层神经元过分依赖某个低层神经元，就会有过拟合发生。在一次正向/反向的过程中，通过随机丢弃一些神经元，迫使高层神经元和其它的一些低层神经元协同工作，可以有效地防止神经元因为接收到过多的同类型参数而陷入过拟合的状态，来提高泛化程度。

丢弃后的结果如下图所示：

<img src="..\Images\16\dropout_after.png">

其中有叉子的神经元在本轮训练中被暂时的封闭了，在下一轮训练中，再随机地封闭一些神经元，同一个神经元也许被连续封闭两次，也许一次都没有被封闭，完全随机。

# 正向计算

正常的隐层计算公式是：

$$
Z = W \cdot X + B \tag{1}
$$

加入随机丢弃步骤后，变成了：

$$
r \sim Bernoulli(p) \tag{2}
$$
$$Y = r * X \tag{3}$$
$$Z = W \cdot Y + B \tag{4}
$$

公式2是得到一个分布概率为p的伯努利分布，伯努利分布在这里可以简单地理解为0、1分布，p=0.5时，会生产与X相同数量的0、1，假设一共10个数，则：
$$
r=[0,0,1,1,0,1,0,1,1,0]
$$
或者
$$
r=[0,1,1,0,0,1,0,1,0,1]
$$
或者其它一些分布。

从公式3，Y将会是X经过r的mask的结果，1的位置保留原x值，0的位置相乘后为0。

# 反向传播

在反向传播时，和Relu函数的反向差不多，需要记住正向计算时得到的mask值，反向的误差矩阵直接乘以这个mask值就可以了。

# 训练和测试阶段的不同

在训练阶段，我们使用正向计算的逻辑。在测试时，不能随机丢弃一些神经元，否则会造成测试结果不稳定，比如某个样本的第一次测试，得到了结果A；第二次测试，得到结果B。由于丢弃的神经元的不同，A和B肯定不相同，就会造成无法解释的情况。

但是如何模拟那些在训练时丢弃的神经元呢？我们任然可以利用训练时的丢弃概率，如下图所示：

<img src="..\Images\16\dropout_neuron.png">

左图为训练时，输入的信号会以概率p存在，如果p=0.6，则会有40%的概率被丢弃，60%的概率存在。

右图为测试/推理时，输入信号总会存在，但是在每个输出上，都应该用原始的权重值，乘以概率p。

# 代码实现

```Python
class DropoutLayer(CLayer):
    def __init__(self, input_size, ratio=0.5):
        self.dropout_ratio = ratio
        self.mask = None
        self.input_size = input_size
        self.output_size = input_size

    def forward(self, input, train=True):
        assert(input.ndim == 2)
        if train:
            self.mask = np.random.rand(*input.shape) > self.dropout_ratio
            self.z = input * self.mask
        else:
            self.z = input * (1.0 - self.dropout_ratio)

        return self.z
       
    def backward(self, delta_in, idx):
        delta_out = self.mask * delta_in
        return delta_out
```

上面的代码中，ratio是丢弃率，如果ratio=0.4，则前面的原理解释中的p=1-0.4=0.6。

另外，我们可以看到，这里的DropoutLayer是作为一个层出现的，而不是寄生在全连接层内部。貌似位置不对，但其实把它看作全连接层之前的处理层就可以了，逻辑上是一样的。

写好Dropout层后，我们在原来的基础上，搭建一个带Dropout层的新网络，如下图：

<img src="..\Images\16\dropout_net.png">

与前面的过拟合的网络相比，只是在每个层之间增加一个Drouput层。用代码理解的话，请看下面的函数：

```Python
ddef DropoutNet(dataReader, num_input, num_hidden, num_output, params):
    net = NeuralNet(params)

    fc1 = FcLayer(num_input, num_hidden, params)
    net.add_layer(fc1, "fc1")
    relu1 = ActivatorLayer(Relu())
    net.add_layer(relu1, "relu1")

    drop1 = DropoutLayer(num_hidden, 0.1)
    net.add_layer(drop1, "dp1")

    fc2 = FcLayer(num_hidden, num_hidden, params)
    net.add_layer(fc2, "fc2")
    relu2 = ActivatorLayer(Relu())
    net.add_layer(relu2, "relu2")
    
    drop2 = DropoutLayer(num_hidden, 0.4)
    net.add_layer(drop2, "dp2")
    
    fc3 = FcLayer(num_hidden, num_hidden, params)
    net.add_layer(fc3, "fc3")
    relu3 = ActivatorLayer(Relu())
    net.add_layer(relu3, "relu3")
    
    drop3 = DropoutLayer(num_hidden, 0.45)
    net.add_layer(drop3, "dp3")
    
    fc4 = FcLayer(num_hidden, num_hidden, params)
    net.add_layer(fc4, "fc4")
    relu4 = ActivatorLayer(Relu())
    net.add_layer(relu4, "relu4")
    
    drop4 = DropoutLayer(num_hidden, 0.5)
    net.add_layer(drop4, "dp4")
    
    fc5 = FcLayer(num_hidden, num_output, params)
    net.add_layer(fc5, "fc5")
    softmax = ActivatorLayer(Softmax())
    net.add_layer(softmax, "softmax")

    net.train(dataReader, checkpoint=1)
    
    net.ShowLossHistory()
``` 

运行程序，最后可以得到这样的损失函数图和验证结果：

<img src="..\Images\16\dropout_result.png">

在损失函数值方面，改善并不大。但是在验证集的准确度验证上，可以达到89%的准确度，而过拟合网络只能得到最高86%的准确度。说明我们的Dropout层确实起到了作用。

```
epoch=197, total_iteration=1773
loss_train=0.4127, accuracy_train=0.870000
loss_valid=0.5220, accuracy_valid=0.860000
epoch=198, total_iteration=1782
loss_train=0.2333, accuracy_train=0.920000
loss_valid=0.5492, accuracy_valid=0.880000
epoch=199, total_iteration=1791
loss_train=0.2833, accuracy_train=0.940000
loss_valid=0.5714, accuracy_valid=0.890000
epoch=199, total_iteration=1799
loss_train=0.2959, accuracy_train=0.920000
loss_valid=0.5242, accuracy_valid=0.880000
time used: 5.2051379680633545
total weights abs sum= 2121.455555980411
total weights = 26520
little weights = 2458
zero weights = 23
```

# 直观理解

关于Dropout，论文中没有给出任何数学解释，Hintion的直观解释和理由如下：

1. 由于每次用输入网络的样本进行权值更新时，隐含节点都是以一定概率随机出现，因此不能保证每2个隐含节点每次都同时出现，这样权值的更新不再依赖于有固定关系隐含节点的共同作用，阻止了某些特征仅仅在其它特定特征下才有效果的情况。

2. 可以将dropout看作是模型平均的一种。对于每次输入到网络中的样本（可能是一个样本，也可能是一个batch的样本），其对应的网络结构都是不同的，但所有的这些不同的网络结构又同时share隐含节点的权值。这样不同的样本就对应不同的模型，是bagging的一种极端情况。

3. 还有一个比较有意思的解释是，Dropout类似于性别在生物进化中的角色，物种为了使适应不断变化的环境，性别的出现有效的阻止了过拟合，即避免环境改变时物种可能面临的灭亡。由于性别是一半一半的比例，所以Dropout中的p一般设置为0.5。

# 相关技术说明

- dropout率的选择

    经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，原因是0.5的时候dropout随机生成的网络结构最多。
    
    dropout也可以被用作一种添加噪声的方法，直接对input进行操作。输入层设为更接近1的数。使得输入变化不会太大。

- 训练过程 

    对参数w的训练进行球形限制(max-normalization)，对dropout的训练非常有用。球形半径c是一个需要调整的参数。可以使用验证集进行参数调优

    dropout自己虽然也很牛，但是dropout、max-normalization、large decaying learning rates and high momentum组合起来效果更好，比如max-norm regularization就可以防止大的learning rate导致的参数blow up。

    使用pretraining方法也可以帮助dropout训练参数，在使用dropout时，要将所有参数都乘以1/p。

#### 思考与练习

1. 分别调整4个dropout层的drop_ratio参数，观察训练结果的变化。

#### 代码位置

ch16, Level4

#### 参考

- http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf
- http://jmlr.org/papers/v15/srivastava14a.html
- https://blog.csdn.net/program_developer/article/details/80737724

