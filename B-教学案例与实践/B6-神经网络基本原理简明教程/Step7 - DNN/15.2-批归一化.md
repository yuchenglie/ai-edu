Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

# 正态分布

正态分布，又叫做高斯分布，

若随机变量X，服从一个位置参数为μ、尺度参数为σ的概率分布，且其概率密度函数为：

$$
f(x)={1 \over \sqrt{2 \pi} \sigma} e^{- {(x-\mu)^2} \over 2\sigma^2} \tag{1}
$$

则这个随机变量就称为正态随机变量，正态随机变量服从的分布就称为正态分布，记作：

$$
X \sim N(\mu,\sigma^2) \tag{2}
$$

当μ=0,σ=1时，称为标准正态分布：

$$X \sim N(0,1) \tag{3}$$

此时公式简化为：

$$
f(x)={1 \over \sqrt{2 \pi}} e^{- {x^2} \over 2} \tag{4}
$$

下图就是三种（μ, σ）组合的函数图像：

<img src="..\Images\15\bn1.png">


# 深度神经网络的挑战

机器学习领域有个很重要的假设：IID独立同分布假设（独立同分布），就是假设训练数据和测试数据是满足相同分布的，这样就能做到通过训练数据获得的模型能够在测试集获得好的效果。

在深度神经网络中，我们可以将每一层视为对输入的信号做了一次变换：

$$
Z = W \cdot X + B \tag{5}
$$

经过这样的变换之后，神经网络的每一层所接受到的输入的分布就可能会发生变化，不再是输入的原始数据所适应的分布了。

比如，在上图中，假设X是服从蓝色曲线的分布，经过公式5后，有可能变成了绿色曲线的分布。

标准正态分布的数值密度占比如下图所示：

<img src="..\Images\15\bn2.png">

有68%的值落在[-1,1]之间，有95%的值落在[-2,2]之间。再回忆一下激活函数Sigmoid的图像，在[-2,2]之外的区域，其导数值很小（小于0.1）:
1. 激活后的值基本接近0或1了，饱和
2. 导数数值小，反向传播的力度很小

<img src="..\Images\8\sigmoid.png">

在前向计算过程中，如果数据整体真的向右偏移了两个单位，在下图中可以看到，在[2,4]范围内的数据（蓝色），都处于激活函数的饱和区（黄色），对于训练就很不利了。

<img src="..\Images\15\bn3.png">

有的人会问，我们在深度学习中不是都用ReLU激活函数吗？那么BN对于ReLU有用吗？下面我们看看ReLU函数的图像：

<img src="..\Images\15\bn4.png">

上图中蓝色为数据分布，黄色为ReLU的激活值，可以看到几乎所有的数据都被前传到了下一层网络中，而没有被小于0的部分剪裁，那么这个网络和线性网络也差不多了，失去了深层网络的能力。

# 批归一化 Batch Normalization

有的书翻译成归一化，有的翻译成正则化，英文Batch Normalization，简称为BatchNorm，或BN。

在上一小节中，我们学习了权重值的初始化，是发生在神经网络的第一层。如果在深度神经网络的每一层，都可以有类似的手段，那么训练效果就会很理想。这就是批归一化的想法的来源。

1. 随着网络的加深，在深层的层接收到的输入的分布和输入的分布可能差距很大，这会在一定程度上影响神经网络I.I.D.（独立同分布）的假设

2. 随着网络的加深，深层的层接受到的数据分布会因为每次训练导致的参数变化而不断变化，那么这些层就既需要去提取特征，又需要去适应不断变化的分布，加大了训练和收敛的难度

深度神经网络随着网络深度加深，训练起来越困难，收敛越来越慢，这是个在DL领域很接近本质的问题。很多论文都是解决这个问题的，比如ReLU激活函数，再比如Residual Network，BN本质上也是解释并从某个不同的角度来解决这个问题的。

BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的，致力于将每一层的输入数据正则化成$N(0,1)$的分布。因次，每次训练的数据必须是mini-batch形式，一般取32，64等数字。

为了不影响网络本身的表达能力，以及可能一些层对非正态分布的数据表现更为优异，给予网络一定的调整能力，在正则化后，额外使用两个参数$\gamma$，$\beta$对分布进行了一定的线性变化。

在实际的工程中，我们把BN当作一个层来看待。

# 前向计算

## 符号表

|符号|数据类型|数据形状|
|:---------:|:-----------:|:---------:|
|$X$| 输入数据矩阵 | [batchSize, Features] |
|$x_i$|输入数据第i个样本| [1, Features] |
|$N$| 经过归一化的数据矩阵 | [batchSize, Features] |
|$n_i$| 经过归一化的单样本 | [1, Features] |
|$\mu_B$| 批数据均值 | [1, Features] |
|$\sigma^2_B$| 批数据方差 | [1, Features] |
|$m$|批样本数量| [1] |
|$\gamma$|线性变换参数| [1, Features] |
|$\beta$|线性变换参数| [1, Features] |
|$Z$|线性变换后的矩阵| [1, Features] |
|$z_i$|线性变换后的单样本| [1, Features] |
|$\delta$| 反向传入的误差 | [batchSize, Features] |

P.S. 无特殊说明，以下乘法为元素乘，即element wise的乘法

在训练过程中，针对每一个batch数据，m是批的大小。进行的操作是，将这组数据正则化，之后对其进行线性变换。

具体的算法步骤是：

$$
\mu_B = {1 \over m}\sum_1^m x_i \tag{6}
$$

$$
\sigma^2_B = {1 \over m} \sum_1^m (x_i-\mu_B)^2 \tag{7}
$$

$$
n_i = {x_i-\mu_B \over \sqrt{\sigma^2_B + \epsilon}} \tag{8}
$$

$$
z_i = \gamma n_i + \beta \tag{9}
$$

其中，$\gamma 和 \beta$是训练出来的，$\epsilon$是防止$\mu_B^2$为0时加的一个很小的数值，通常为1e-5。

# 计算图（示意）

下图是一张示意的计算图，用于帮助我们搞清楚正向和反向的过程：

<img src="..\Images\15\bn5.png">

$X1,X2,X3$表示三个样本（实际上一般用32，64这样的批大小），我们假设每个样本只有一个特征值（否则X将会是一个样本数乘以特征值数量的矩阵）。

# 反向传播

首先假设已知从上一层回传给BN层的误差矩阵是：

$$\delta = {dJ \over dZ}，\delta_i = {dJ \over dz_i} \tag{10}$$

## 求BN层参数梯度

则根据公式9，求$\gamma 和 \beta$的梯度：

$${dJ \over d\gamma} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\gamma}=\sum_{i=1}^m \delta_i \cdot n_i \tag{11}$$

$${dJ \over d\beta} = \sum_{i=1}^m {dJ \over dz_i}{dz_i \over d\beta}=\sum_{i=1}^m \delta_i \tag{12}$$

注意$\gamma和\beta$的形状与批大小无关，只与特征值数量有关，我们假设特征值数量为1，所以它们都是一个标量。在从计算图看，它们都与N,Z的全集相关，而不是某一个样本，因此会用求和方式计算。

## 求BN层的前传误差矩阵

下述所有乘法都是element-wise的矩阵点乘，不再特殊说明。

从正向公式中看，对z有贡献的数据链是：

- $z_i \leftarrow n_i \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \mu_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow x_i$
- $z_i \leftarrow n_i \leftarrow \sigma^2_B \leftarrow \mu_B \leftarrow x_i$


从公式8，9：

$$
{dJ \over dx_i} = {dJ \over d n_i}{d n_i \over dx_i} + {dJ \over d \sigma^2_B}{d \sigma^2_B \over dx_i} + {dJ \over d \mu_B}{d \mu_B \over dx_i} \tag{13}
$$

公式13的右侧第一部分（与全连接层形式一样）：

$$
{dJ \over d n_i}=  {dJ \over dz_i}{dz_i \over dn_i} = \delta_i \cdot \gamma\tag{14}
$$

上式等价于：

$$
{dJ \over d N}= \delta \cdot \gamma\tag{14}
$$

公式14中，我们假设样本数为64，特征值数为10，则得到一个64x10的结果矩阵（因为1x10的矩阵会被广播为64x10的矩阵）：

$$\delta^{(64 \times 10)} \odot \gamma^{(1 \times 10)}=R^{(64 \times 10)}$$

公式13的右侧第二部分，从公式8：
$$
{d n_i \over dx_i}={1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{15}
$$

公式13的右侧第三部分，从公式8（注意$\sigma^2_B$是个标量，而且与X,N的全集相关，要用求和方式）：

$$
{dJ \over d \sigma^2_B} = \sum_{i=1}^m {dJ \over d n_i}{d n_i \over d \sigma^2_B} 
$$
$$
= -{1 \over 2}(\sigma^2_B + \epsilon)^{-3/2}\sum_{i=1}^m {dJ \over d n_i} \cdot (x_i-\mu_B) \tag{16}
$$


公式13的右侧第四部分，从公式7：
$$
{d \sigma^2_B \over dx_i} = {2(x_i - \mu_B) \over m} \tag{17}
$$

公式13的右侧第五部分，从公式7，8：

$$
{dJ \over d \mu_B}={dJ \over d n_i}{d n_i \over d \mu_B} + {dJ \over  d\sigma^2_B}{d \sigma^2_B \over d \mu_B} \tag{18}
$$

公式18的右侧第二部分，根据公式8：

$$
{d n_i \over d \mu_B}={-1 \over \sqrt{\sigma^2_B + \epsilon}} \tag{19}
$$

公式18的右侧第四部分，根据公式7（$\sigma^2_B和\mu_B$与全体$x_i$相关，所以要用求和）：

$$
{d \sigma^2_B \over d \mu_B}=-{2 \over m}\sum_{i=1}^m (x_i- \mu_B) \tag{20}
$$

所以公式18是：

$$
{dJ \over d \mu_B}=-{\delta \cdot \gamma \over \sqrt{\sigma^2_B + \epsilon}} - {2 \over m}{dJ \over d \sigma^2_B}\sum_{i=1}^m (x_i- \mu_B) \tag{18}
$$

公式13的右侧第六部分，从公式6：

$$
{d \mu_B \over dx_i} = {1 \over m} \tag{21}
$$

所以，公式13最后是这样的：

$$
{dJ \over dx_i} = {\delta \cdot \gamma \over \sqrt{\sigma^2_B + \epsilon}} + {dJ \over d\sigma^2_B} \cdot {2(x_i - \mu_B) \over m} + {dJ \over d\mu_B} \cdot {1 \over m} \tag{13}
$$

# 测试和推理时的归一化方法

在测试过程或推理时，我们只有一张或者几张图的数据，根本没有mini-batch的概念，无法算出来正确的均值。因此，我们使用的均值和方差数据是在训练过程中样本值的平均。也就是：

$$
E[x] = E[\mu_B]
$$
$$
Var[x] = {m \over m-1} E[\sigma^2_B]
$$

这就需要我们把所有批次的$\mu和\sigma$都记录下来，然后在最后训练完毕时（或做测试时）平均一下。

另外一种做法是使用类似动量的方式，训练时，加权平均每个批次的值，权值$\alpha$可以为0.9：

$$m_{t} = \alpha \cdot m_{t-1} + (1-\alpha) \cdot \mu_t$$
$$v_{t} = \alpha \cdot v_{t-1} + (1-\alpha) \cdot \sigma_t$$

测试或推理时，直接使用$m_t和v_t$的值即可。

# 优点

- 没有它之前，需要小心的调整学习率和权重初始化，但是有了BN可以放心的使用大学习率，但是使用了BN，就不用小心的调参了，较大的学习率极大的提高了学习速度
- Batchnorm本身上也是一种正则的方式，可以代替其他正则方式如L2, Dropout等

# 参考

https://arxiv.org/abs/1502.03167


