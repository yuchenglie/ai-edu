Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可

## 14.3 二分类试验 - 双弧形非线性二分类

### 14.3.1 搭建模型

同样是一个双层神经网络，但是最后一层要接一个Logistic二分类函数来完成二分类任务：

<img src='../Images/14/bc_net.png'/>

```Python
if __name__ == '__main__':
    dr = XOR_DataReader()
    dr.ReadData()
    
    num_input = 2
    num_hidden = 2
    num_output = 1

    max_epoch = 10000
    batch_size = 1
    learning_rate = 0.1
    eps = 0.001

    params = CParameters(
        learning_rate, max_epoch, batch_size, eps,
        LossFunctionName.CrossEntropy2,
        InitialMethod.Xavier, 
        OptimizerName.SGD)

    net = NeuralNet(params, "XOR")

    fc1 = FcLayer(num_input, num_hidden, params)
    net.add_layer(fc1, "fc1")
    sigmoid1 = ActivatorLayer(Sigmoid())
    net.add_layer(sigmoid1, "sigmoid1")
    
    fc2 = FcLayer(num_hidden, num_output, params)
    net.add_layer(fc2, "fc2")
    sigmoid2 = ClassificationLayer(Sigmoid())
    net.add_layer(sigmoid2, "sigmoid2")

    net.train(dr, checkpoint=100, need_test=True)
    net.ShowLossHistory()
    ShowResult2D(net)
```

同前面的章节中解决异或问题的方案一样，这里采用了一个两层的结构：一个隐层，一个输出层。其中：
- 输入层神经元数为2
- 隐层的神经元数为2，使用Sigmoid激活函数
- 由于是二分类任务，所以输出层只有一个神经元，用Sigmoid做二分类函数

其它参数：
- max_epoch=10000，最多训练10000轮
- batch_size=1，每次用一个数据训练
- learning_rate=0.1
- eps停止条件=0.001，根据经验，当损失函数值到达0.001时，就能很好地完成异或的二分类任务
- checkpoint=100，每隔100个epoch计算一次损失函数值
- need_test=True，最后需要用XTest数据集测试一下是否和YTest的值相等

### 14.3.2 运行结果

<img src='../Images/14/ch10_loss.png'/>

再看下面的打印输出结果：

```
......
epoch=459, total_iteration=33119
loss_train=0.029127, accuracy_train=1.000000
loss_valid=0.048596, accuracy_valid=1.000000
time used: 1.7264153957366943
testing...
1.0
1.0
```
最后的testing...的结果是1.0，表示100%正确，这初步说明mini框架在这个基本case上工作得很好。

<img src='../Images/14/ch10_result.png'/>

### 代码位置

ch14, Level3
