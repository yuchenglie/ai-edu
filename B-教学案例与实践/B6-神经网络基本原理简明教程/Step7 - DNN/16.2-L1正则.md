Copyright © Microsoft Corporation. All rights reserved.
  适用于[License](https://github.com/Microsoft/ai-edu/blob/master/LICENSE.md)版权许可


# 另一个朴素的想法

我们把熟悉的等高线图拿出来再看一眼：

<img src="..\Images\16\regular0.png">

假设只有两个参数需要学习，那么这两个参数的损失函数就构成了上面的等高线图。

在L2正则中，我们想办法让W的值都变得比较小，这样就不会对特征敏感。但是也会杀敌一千，自损八百，连有用特征一起被忽视掉了。那么换个思路，能不能让神经网络自动选取有用特征，忽视无用特征呢？也就是让有用特征的权重比较大，让无用特征的权重比较小，甚至为0。

用上面的图举例，公式为：

$$z=w1 \cdot x1 + w2 \cdot x2 + b$$

假设x1是无用特征，想办法让w1变得很小或者是0，就会得到比较满意的模型。这种想法在只有两个特征值时不明显，甚至不正确，但是当特征值有很多时，比如MNIST数据中的784个特征，肯定有些是非常重要的特征，有些是没什么大用的特征。

# 基本数学知识

## 拉普拉斯分布

$$f(x)=\frac{1}{2b}exp(-\frac{|x-\mu|}{b})$$
$$= \frac{1}{2b} \begin{cases} exp(\frac{x-\mu}{b}), & x \lt \mu \\ exp(\frac{\mu-x}{b}), & x \gt \mu \end{cases}$$


## L0范数与L1范数

L0范数是指向量中非0的元素的个数。如果我们用L0范数来规则化一个参数矩阵W的话，就是希望W的大部分元素都是0，即让参数W是稀疏的。

L1范数是指向量中各个元素绝对值之和，也叫“稀疏规则算子”（Lasso regularization）。为什么L1范数会使权值稀疏？有人可能会这样给你回答“它是L0范数的最优凸近似”。实际上，还存在一个更美的回答：任何的规则化算子，如果他在Wi=0的地方不可微，并且可以分解为一个“求和”的形式，那么这个规则化算子就可以实现稀疏。W的L1范数是绝对值，|w|在w=0处是不可微。

为什么L0和L1都可以实现稀疏，但常用的为L1？一是因为L0范数很难优化求解，二是L1范数是L0范数的最优凸近似，而且它比L0范数要容易优化求解。所以大家才把目光转于L1范数。

综上，L1范数和L0范数可以实现稀疏，L1因具有比L0更好的优化求解特性而被广泛应用。

# L1正则化

假设：
- W参数服从拉普拉斯分布，即$w_j \sim Laplace(0,b)$
- Y服从高斯分布，即$y_i \sim N(w^Tx_i,\sigma^2)$

贝叶斯最大后验估计：
$$
argmax_wL(w) = ln \prod_i^n {1 \over \sigma\sqrt{2 \pi}}exp(-\frac{1}{2}(\frac{y_i-w^Tx_i}{\sigma})^2) \cdot \prod_j^m{\frac{1}{2b}exp(-\frac{\lvert w_j \rvert}{b})}
$$
$$
=-\frac{1}{2\sigma^2}\sum_i^n(y_i-w^Tx_i)-\frac{1}{2b}\sum_j^m{\lvert w_j \rvert}-n\ln\sigma\sqrt{2\pi}-m\ln b\sqrt{2\pi}
$$

则损失函数J(w)的最小值为：

$$
argmin_wJ(w) = \sum_i^n(y_i-w^Tx_i)^2+\lambda\sum_j^m{\lvert w_j \rvert}
$$


我们仍以两个参数为例，如果函数：
$$L = \lvert w_1 \rvert + \lvert w_2 \rvert$$
则它在二维平面上是一个菱形。下图中三个菱形，是因为惩罚因子的数值不同，越大的话，菱形面积越小，惩罚越厉害。

<img src="..\Images\16\regular1.png">

当菱形面积从小变大时（惩罚系数从大变小），假设从0.9变到0.8，那么与绿色损失函数曲线先接触的是

以最大的那个菱形区域为例，它与损失函数等高线有多个交点，都可以作为最终解，但是其中红色顶点是损失函数值最小的，因此它是最优解。从另一方面看，假设解空间从椭圆中心向外一层层扩充，最有可能先接触到的，就是菱形的顶点。

菱形顶点的含义具有特殊性，即$W=[w2, 0]$，也就是w1的值为0。扩充到高维空间，这个菱形像个刺猬，其顶点就是只有少数的参数有值，其它参数都为0。这就是所谓的稀疏解。

# 损失函数的变化

假设我们以前使用的损失函数为$J_0$，则新的损失函数变成：

$$J = J_0 + \frac{\lambda}{m} \sum_i^m \lvert w_i \rvert$$

代码片段如下：

```Python
  regular_cost = 0
  for i in range(self.layer_count-1,-1,-1):
      layer = self.layer_list[i]
      if isinstance(layer, FcLayer):
          if regularName == RegularMethod.L1:
              regular_cost += np.sum(np.abs(layer.weights.W))
          elif regularName == RegularMethod.L2:
              regular_cost += np.sum(np.square(layer.weights.W))
      # end if
  # end for
  return regular_cost * self.params.lambd
```

可以看到L1部分的代码，先求绝对值，再求和。那个分母上的m是在下一段代码中处理的，因为在上一段代码中，没有任何样本数量的信息。

```Python
loss_train = self.lossFunc.CheckLoss(train_y, self.output)
loss_train += regular_cost / train_y.shape[1]
```
train_y.shape[1]就是样本数量。

# 反向传播的变化

假设一个两层的神经网络，其前向过程是：

$$Z1=W1 \cdot X + B1$$
$$A1 = Sigmoid(Z1)$$
$$Z2=W2 \cdot A1 + B2$$
$$J(w,b) = J_0 + \lambda (\lvert W1 \rvert+\lvert W2 \rvert)$$

则反向过程为：

$$dW2=\frac{dJ}{dW2}=\frac{dJ}{dZ2}\frac{dZ2}{dW2}+\frac{dJ}{dW2}$$
$$=dZ2 \cdot A1^T+\lambda \odot sign(W2)$$
$$dW1= dZ1 \cdot X^T + \lambda \odot sign(W1) $$

从上面的公式中可以看到，正则项在方向传播过程中，唯一影响的就是求W的梯度时，要增加一个$\lambda \odot sign(W)$，sign是符号函数，返回该值的符号，即1或-1。所以，我们可以修改FullConnectionLayer.py中的反向传播函数如下：

```Python
def backward(self, delta_in, idx):
    dZ = delta_in
    m = self.x.shape[1]
    if self.regular == RegularMethod.L2:
        self.weights.dW = (np.dot(dZ, self.x.T) + self.lambd * self.weights.W) / m
    elif self.regular == RegularMethod.L1:
        self.weights.dW = (np.dot(dZ, self.x.T) + self.lambd * np.sign(self.weights.W)) / m
    else:
        self.weights.dW = np.dot(dZ, self.x.T) / m
    # end if
    self.weights.dB = np.sum(dZ, axis=1, keepdims=True) / m
    ......
```
符号函数的效果如下：
```Python
>>> a=np.array([1,-1,2,0])
>>> np.sign(a)
>>> array([ 1, -1,  1,  0])
```
# 运行结果

在主过程中，修改超参实例如下：
```Python
params = CParameters(
    learning_rate, max_epoch, batch_size, eps,
    LossFunctionName.CrossEntropy3, 
    InitialMethod.Xavier, 
    OptimizerName.SGD,
    RegularMethod.L1, lambd=0.15)
```
设置L1正则方法，系数为0.15。

<img src="..\Images\16\L1_result.png">

从图上看，无论是损失函数值还是准确率，在训练集上都没有表现得那么夸张了，不会极高（到100%）或者极低（到0.001）。这说明过拟合的情况得到了抑制。

```
epoch=197, total_iteration=1773
loss_train=0.6456, accuracy_train=1.000000
loss_valid=1.1983, accuracy_valid=0.860000
epoch=198, total_iteration=1782
loss_train=0.6446, accuracy_train=1.000000
loss_valid=1.1917, accuracy_valid=0.860000
epoch=199, total_iteration=1791
loss_train=0.6445, accuracy_train=1.000000
loss_valid=1.1851, accuracy_valid=0.860000
epoch=199, total_iteration=1799
loss_train=0.6630, accuracy_train=0.990000
loss_valid=1.2366, accuracy_valid=0.850000
time used: 2.5629215240478516
total weights abs sum= 391.90651603918013
total weights = 26520
little weights = 22935
zero weights = 12384
```

从输出结果分析：

1. 权重值的绝对值和等于391.26，远小于过拟合时的1719
2. 较小的权重值（小于0.01）的数量为22935个，远大于过拟合时的2810个
3. 趋近于0的权重值（小于0.0001）的数量为12384个，大于过拟合时的25个。

可以看到L1的模型权重非常稀疏（趋近于0的数量很多）。那么参数稀疏有什么好处呢？有两点：

1. 特征选择(Feature Selection)：

    大家对稀疏规则化趋之若鹜的一个关键原因在于它能实现特征的自动选择。一般来说，x的大部分元素（也就是特征）都是和最终的输出y没有关系或者不提供任何信息的，在最小化目标函数的时候考虑x这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确y的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。

2. 可解释性(Interpretability)：

    另一个青睐于稀疏的理由是，模型更容易解释。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：$y=w1*x1+w2*x2+…+w1000*x1000+b$（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。但如果1000个wi都非0，医生面对这1000种因素,无法采取针对性治疗。


# L1和L2的比较

|比较项|无正则项|L2|L1|
|---|---|---|---|
|代价函数|$J_0$|$J_0+\lambda \lvert w \rvert^2_2$|$J_0+\lambda \lvert w \rvert_1$|
|梯度计算|$dw$|$dw+\lambda \cdot w$|$dw+\lambda \cdot sign \lvert w \rvert$|
|训练集损失值|0.001|1.345|0.063|
|验证集损失值|1.009|1.643|1.236|
|训练集准确率|100%|97%|99%|
|验证集准确率|83%|89%|85%|
|总参数数量|26520|26520|26520|
|0参数数量|25|455|12384|
|小值参数数量|2810|15403|22935|
|作用|N/A|使得权重值变小，对非主流特征不敏感|使得权重值很多为0，矩阵稀疏，形成特征选择|
|使用场景|N/A|大部分特征都能起作用时|只有少数特征有作用时|

实际上在上述试验中，L1/L2两个参数值都选得偏小，可以看到训练集的准去率还是很高，有过拟合的倾向。但为了保证验证集准确度不降下来，所以惩罚力度较弱。大家可以自己试验把惩罚参数调高。

#### 代码位置

ch16, Level1

#### 参考资料

https://blog.csdn.net/red_stone1/article/details/80755144

https://www.jianshu.com/p/c9bb6f89cfcc
